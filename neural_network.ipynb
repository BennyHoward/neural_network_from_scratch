{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "% Macro to define the angle bracket convension\n",
    "% Usage:\n",
    "%    \\agl{subject}{epoch}{training datapoint}{layer}\n",
    "% Example:\n",
    "%    \\agl{x}{e}{t}{l}\n",
    "\\newcommand{\\agl}[4]{\\langle_{#2}{#1}_{#4}^{#3}\\rangle}\n",
    "\\newcommand{\\x}[1]{\\agl{\\vec{x}}{}{\\phantom{|}#1}{}}\n",
    "\\newcommand{\\y}[1]{\\agl{\\vec{y}}{}{\\phantom{|}#1}{}}\n",
    "\\newcommand{\\I}[2]{\\agl{I}{#1}{}{l}}\n",
    "\\newcommand{\\J}[2]{\\agl{J}{#1}{}{l}}\n",
    "\\newcommand{\\W}[2]{\\agl{\\bf{W}}{#1}{}{#2}}\n",
    "\\newcommand{\\b}[2]{\\agl{\\vec{b}}{#1}{}{#2}}\n",
    "\\newcommand{\\z}[3]{\\agl{\\vec{z}}{#1}{\\phantom{|}#2}{#3}}\n",
    "\\newcommand{\\a}[3]{\\agl{\\vec{a}}{#1}{\\phantom{|}#2}{#3}}\n",
    "\\newcommand{\\pred}[2]{\\agl{\\hat{y}}{#1}{\\phantom{|}#2}{}}\n",
    "\\newcommand{\\c}[2]{\\agl{C}{#1}{#2}{}}\n",
    "\\newcommand{\\d}[3]{\\agl{\\vec{\\delta}}{#1}{\\phantom{|}#2}{#3}}\n",
    "\\newcommand{\\C}[1]{\\agl{\\bar{C}}{#1}{}{}}\n",
    "\\newcommand{\\dcdW}[3]{\\frac{\\partial\\c{#1}{#2}}{\\partial\\W{#1}{#3}}}\n",
    "\\newcommand{\\dcdb}[3]{\\frac{\\partial\\c{#1}{#2}}{\\partial\\b{#1}{#3}}}\n",
    "\\newcommand{\\dCdW}[2]{\\frac{\\partial\\C{#1}}{\\partial\\W{#1}{#2}}}\n",
    "\\newcommand{\\dCdb}[2]{\\frac{\\partial\\C{#1}}{\\partial\\b{#1}{#2}}}\n",
    "$$\n",
    "## Explanation of the Mathmatics Notation\n",
    "\n",
    "The mathmatics required to do this isn't trivial, but what makes it complicated are the number of variables to track.  When going through the math you find that you'll quickly run out of places to denote indicies, you quickly lose track of where you are and whether your dealing with a vector, scalar, matrix, etc.  \n",
    "\n",
    "Here we'll define a new convention which will allow us better understand the mathmatics and communicate the concepts via the language of mathmatics.  The theories and math behind this is complicated enough, why complicate it further with confusing notation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle Bracket Convension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "Because of the large amount of items we'll be tracking, we will be running out of spaces to put indexes.  Also, since we don't want to confuse with existing convensions, such as superscripts for exponentiation, we'll be using angle brackets to indicate a custom convension.  \n",
    "\n",
    "For example the following: $x^t$, would indicate $x$ raised to the power of $t$. \n",
    "\n",
    "But we'll be using the following: $\\agl{x}{}{t}{}$ to indicate $x$ at index $t$ to help clarify that we mean to index something rather than raise to a power.  \n",
    "\n",
    "Therefore, $\\agl{x}{}{t}{}^t$ would mean $x$ at index $t$ raised to the power of $t$.  \n",
    "\n",
    "Going further, we'll also define our convention to indicate that the position of the superscripts and subscripts to indicate which index we mean.  \n",
    "Here, we'll use the subscript that appears before the letter to indicate the training epoch, the superscript will be used to indicate the training datapoint, and the subscript which appears after the letter to indicate the layer in the network.\n",
    "\n",
    "For example: $\\agl{x}{e}{t}{l}$ means $x$ in training epoch $e$, for training datapoint $t$, at layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalars, Vectors, and Matricies\n",
    "\n",
    "In addition to the indicies, we'll also use certain letters to help us destinguish between scalars, vectors, and matricies.  This will help keep our equations easy to understand.\n",
    "- ALL matricies will be denoted with a capital letter with a bold fontface.  \n",
    "  For example: $\\bf M$  \n",
    "\n",
    "- ALL vectors will be denoted with a lowercase letter with an arrow or hat accent mark.  \n",
    "  For example: $\\vec v$, and $\\hat v$\n",
    "\n",
    "- If it's not indicated as a matrix or vector, then it can safely be assumed to be a scalar.  \n",
    "  For example:\n",
    " - $M$ is not a matrix, but a scalar, because though it's a capital letter, it's NOT bold. \n",
    " - $v$ and $\\bar v$ are not a vectors, but a scalars, because the lack of an accent mark or the accent mark is NOT an arrow or hat, but a bar.\n",
    " - and of course $x$ would be a scalar.\n",
    " \n",
    "#### Putting it all together\n",
    " \n",
    "Combining the notations we'll end up with something like $\\agl{\\bf M}{e}{t}{l}$, means matix $\\bf M$ in training epoch $e$, for training datapoint $t$, at layer $l$.\n",
    "\n",
    "Outside this angle bracket convenstion, regular mathmatical convension applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix and Vector indexing Convension\n",
    "\n",
    "In addition to the angle bracket convension, we'll use another convension to indicate if we are refering to an index of a matrix or vector.  It'll be denoted by a letter or an angle bracket expression wrapped by square brackets, followed by a subscript.  \n",
    "\n",
    "A single subscript indicates a particular element of a vector and two subscripts seperated by a comma indicates a particular element of a matrix.  \n",
    "\n",
    "For example:\n",
    "\n",
    "- $[{\\bf M}]_{2,3}$ means element on row 2, column 3 in matrix $\\bf M$\n",
    "- $[{\\vec v}]_{2}$ means element on row 2 in vector $\\vec v$\n",
    "- However, something like $[{\\vec v}]_{2,3}$ is invalid because $\\vec v$ is a vector and doesn't have another dimension for $3$ to refer to.\n",
    "- And $[{\\bf M}]_{2}$ is also invalid because we need to specify a column on the matrix.\n",
    "- Also, it goes without saying that, $[x]_{2}$ and $[x]_{2,3}$ would be invalid because $x$ is a scalar.\n",
    "\n",
    "Don't forget, this convension also appies to bracket convension too: \n",
    "\n",
    "- $[\\agl{\\bf M}{}{}{}]_{2,3}$ means element on row 2, column 3 in matrix $\\agl{\\bf M}{}{}{}$\n",
    "- $[\\agl{\\vec v}{}{}{}]_{2}$ means element on row 2 in vector $\\agl{\\vec v}{}{}{}$\n",
    "- However, something like $[\\agl{\\vec v}{}{}{}]_{2,3}$ is invalid because $\\agl{\\vec v}{}{}{}$ is a vector and doesn't have another dimension for $3$ to refer to.\n",
    "- And $[\\agl{\\bf M}{}{}{}]_{2}$ is also invalid because we need to specify a column on the matrix.\n",
    "- Also, it goes without saying that, $[\\agl{x}{}{}{}]_{2}$ and $[\\agl{x}{}{}{}]_{2,3}$ would be invalid because $\\agl{x}{}{}{}$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "In order to track the increadible amount of items we'll define how we'll denote those items to help of keep track.  \n",
    "Yes, there is a total of 25 items to track at any given point, but with a properly organized system this will become a simple task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indecies\n",
    "\n",
    "- A particular training epoch will be indexed by $e$\n",
    "- A particular training datapoint will be indexed by $t$\n",
    "- A particular layer in the network will be indexed by $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "\n",
    "- The total number of training epochs will be denoted by $E$\n",
    "- The total number of training datapoints will be denoted by $T$\n",
    "- The total number of layers in the network will be denoted by $L$\n",
    "- The total number of rows at layer $l$ in training epoch $e$ will be denoted by $\\I{e}{l}$\n",
    "- The total number of columns at layer $l$ in training epoch $e$ will be denoted by $\\J{e}{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and Labels\n",
    "\n",
    "- Feature vector of a given training datapoint will be denoted by $\\x{t}$\n",
    "- Label vector corresponding to the feature vector of a given training datapoint will be denoted by $\\y{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations, Weights, Biases, etc.\n",
    "\n",
    "- The activation function will be denoted by $f(z)$\n",
    "There are many activation functions that can be used such as $tanh$ (hyperbolic tangent), $ReLU$ (rectified linear unit), etc., however in this case we'll be using the sigmoid (a.k.a the logistic function).  \n",
    "The sigmoid is defined to be:\n",
    "$$f(z) = \\frac{1}{1+exp(-z)}$$\n",
    "- The derivative of the activation function will be denoted by $f^\\prime(z)$\n",
    "The derivative of the sigmoid is defined to be:\n",
    "$$f^\\prime(z) = \\frac{\\partial{f(z)}}{\\partial{z}} = f(z)\\cdot(1-f(z))$$\n",
    "***Note***: *We'll be using this for backpropagation.*\n",
    "- The weight matrix at layer $l$ in training epoch $e$ will be denoted by $\\W{e}{l}$\n",
    "- The bias vector at layer $l$ in training epoch $e$ will be denoted by $\\b{e}{l}$\n",
    "- Pre-activation vector at layer $l$, for training datapoint $t$, in epoch $e$ will be denoted by $\\z{e}{t}{l}$\n",
    "- Activation vector at layer $l$, for training datapoint $t$, in epoch $e$ will be denoted by $\\a{e}{t}{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions, evaluations, and adjustments\n",
    "\n",
    "- Predicted label vector for datapoint $t$ in epoch $e$ will be denoted by $\\pred{e}{t}$\n",
    "- The cost for training datapoint $t$ in epoch $e$ will be denoted by $\\c{e}{t}$\n",
    "- The error vector at layer $l$, for training datapoint $t$, in epoch $e$ will be denoted by $\\d{e}{t}{l}$\n",
    "- The average cost for all training datapoints in epoch $e$ will be denoted by $\\C{e}$\n",
    "- The adjustment to the weight matrix needed for the next epoch to lower the cost for the particular training datapoint $t$ will be denoted by $\\dcdW{e}{t}{l}$\n",
    "- The adjustment to the bias vector needed for the next epoch to lower the cost for the particular training datapoint $t$ will be denoted by $\\dcdb{e}{t}{l}$\n",
    "- The adjustment to the weight matrix needed for the next epoch to lower the average cost across all training datapoints will be denoted by $\\dCdW{e}{l}$\n",
    "- The adjustment to the bias vector needed for the next epoch to lower the average cost across all training datapoints will be denoted by $\\dCdb{e}{l}$\n",
    "- The learning rate hyperparameter will be denoted by $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sanity Checks\n",
    "\n",
    "Run the following cells in this section to run some system checks.  \n",
    "If a potential issue is detected, warnings will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "import logging\n",
    "\n",
    "if sys.executable != subprocess.run(['which', 'python'], stdout=subprocess.PIPE, text=True).stdout.strip():\n",
    "    logging.warning('\\n    '.join(['',\n",
    "        'Kernel not set to use the same Python runtime as this notebook.',\n",
    "         'Change the kernel or else the remaining cells may not properly execute.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Funcitons\n",
    "\n",
    "Before we dive directly in the code, let's first create a few utility functions.  \n",
    "These will help us load the raw data from the `MNIST_digits` folder and visualize the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed to force replicable results\n",
    "np.random.seed(0)\n",
    "# Allow plots to be displayed inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def load_mnist_training_data():\n",
    "    # Load the training labels\n",
    "    training_labels = None\n",
    "    training_labels_path = os.getcwd() + '/MNIST_digits/train-labels-idx1-ubyte'\n",
    "    with open(training_labels_path, 'rb') as training_labels_file:\n",
    "        training_labels = np.frombuffer(\n",
    "            training_labels_file.read(\n",
    "                os.path.getsize(\n",
    "                    training_labels_path))[8:],\n",
    "            dtype=np.uint8)\n",
    "    # Load the training images\n",
    "    training_images = None\n",
    "    training_images_path = os.getcwd() + '/MNIST_digits/train-images-idx3-ubyte'\n",
    "    with open(training_images_path, 'rb') as training_images_file:\n",
    "        training_images = np.frombuffer(\n",
    "            training_images_file.read(\n",
    "                os.path.getsize(\n",
    "                    training_images_path))[16:],\n",
    "            dtype=np.uint8).reshape(60000, 28, 28)\n",
    "    # Return the raw Zipped result\n",
    "    return list(zip(training_labels, training_images))\n",
    "\n",
    "def load_mnist_testing_data():\n",
    "    # Load the testing labels\n",
    "    testing_labels = None\n",
    "    testing_labels_path = os.getcwd() + '/MNIST_digits/t10k-labels-idx1-ubyte'\n",
    "    with open(testing_labels_path, 'rb') as testing_labels_file:\n",
    "        testing_labels = np.frombuffer(\n",
    "            testing_labels_file.read(\n",
    "                os.path.getsize(\n",
    "                    testing_labels_path))[8:],\n",
    "            dtype=np.uint8)\n",
    "    # Load the testing images\n",
    "    testing_images = None\n",
    "    testing_images_path = os.getcwd() + '/MNIST_digits/t10k-images-idx3-ubyte'\n",
    "    with open(testing_images_path, 'rb') as testing_images_file:\n",
    "        testing_images = np.frombuffer(\n",
    "            testing_images_file.read(\n",
    "                os.path.getsize(testing_images_path))[16:],\n",
    "            dtype=np.uint8).reshape(10000,28,28)\n",
    "    # Return the raw Zipped result\n",
    "    return list(zip(testing_labels, testing_images))\n",
    "\n",
    "def view_image(image):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.axis('off')\n",
    "    imgplot = ax.imshow(image, cmap = mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    plt.show()\n",
    "\n",
    "def view_image_data(image):\n",
    "    for image_row in image:\n",
    "        print(' '.join([\n",
    "            str(image_row_item).rjust(3, ' ')\n",
    "            for image_row_item in image_row ]))\n",
    "\n",
    "# Training image data\n",
    "# A list of tuples of two items, where the first index of the tuple is the label (number) and the second is the image data (2D numpy array of grayscale image data) \n",
    "TRAINING_DATA = load_mnist_training_data()\n",
    "# Training image data\n",
    "# A list of tuples of two items, where the first index of the tuple is the label (number) and the second is the image data (2D numpy array of grayscale image data) \n",
    "TESTING_DATA = load_mnist_testing_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `load_mnist_training_data` function loads the training data from the `MNIST_digits`.\n",
    "- The `load_mnist_testing_data` function loads the testing data from the `MNIST_digits`.\n",
    "- The `view_image` function displays the image inline in the notebook.\n",
    "- The `view_image_data` function displays the actual 2D numpy array in a formatted way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Data\n",
    "\n",
    "Let's look at some data.  \n",
    "Let's look at `TRAINING_DATA[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, image = TRAINING_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING_DATA[1] is the number 0\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING_DATA[1] is the number', title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image \n",
    "Here is what you see, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...here is what the computer \"sees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  51 159 253 159  50   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252 253 122   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228  47  79 255 168   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0   0   0   0   0 253 252 165   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0  57 252 252  63   0   0   0   0   0   0   0   0   0 253 252 195   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253 196   0   0   0   0   0\n",
      "  0   0   0   0   0   0  76 246 252 112   0   0   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0\n",
      "  0   0   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135 253 186  12   0   0   0   0   0\n",
      "  0   0   0   0   0   0  85 252 223   0   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165 252 173   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0  86 253 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253 223 167  56   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0  28 199 252 252 253 252 252 233 145   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "view_image_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations to Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, image = TRAINING_DATA[0];\n",
    "X = np.matrix(image.reshape(784, 1)).astype(np.float64)\n",
    "y = np.matrix([1 if x == label else 0 for x in range(10)]).reshape(10, 1)\n",
    "\n",
    "W1 = np.matrix(np.random.randn(16, 784))\n",
    "W2 = np.matrix(np.random.randn(16, 16))\n",
    "W3 = np.matrix(np.random.randn(10, 16))\n",
    "\n",
    "b1 = np.matrix(np.random.randn(16, 1))\n",
    "b2 = np.matrix(np.random.randn(16, 1))\n",
    "b3 = np.matrix(np.random.randn(10, 1))\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.63974787],\n",
       "        [0.98814477],\n",
       "        [0.1888836 ],\n",
       "        [0.28189434],\n",
       "        [0.80956   ],\n",
       "        [0.5963427 ],\n",
       "        [0.02977397],\n",
       "        [0.98059997],\n",
       "        [0.03482169],\n",
       "        [0.90439058]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0 = f(X)\n",
    "\n",
    "z1 = W1 * A0 + b1\n",
    "A1 = f(z1)\n",
    "\n",
    "z2 = W2 * A1 + b2\n",
    "A2 = f(z2)\n",
    "\n",
    "z3 = W3 * A2 + b3\n",
    "A3 = f(z3)\n",
    "\n",
    "y_hat = A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(z):\n",
    "    return np.multiply(f(z), (1 - f(z)))\n",
    "\n",
    "delta3 = np.multiply(y_hat - y, f_prime(z3))\n",
    "delta2 = np.multiply(W3.T * delta3, f_prime(z2))\n",
    "delta1 = np.multiply(W2.T * delta2, f_prime(z1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.mean(np.power(y - y_hat, 2))\n",
    "\n",
    "W1 -= delta1 * A0.T\n",
    "W2 -= delta2 * A1.T\n",
    "W3 -= delta3 * A2.T\n",
    "\n",
    "b1 -= delta1\n",
    "b2 -= delta2\n",
    "b3 -= delta3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At training epoch 10 the cost is now 3.78797270260658e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def feedforward(X, f, W1, W2, W3):\n",
    "    A0 = f(X)\n",
    "    z1 = W1 * A0 + b1\n",
    "    A1 = f(z1)\n",
    "    z2 = W2 * A1 + b2\n",
    "    A2 = f(z2)\n",
    "    z3 = W3 * A2 + b3\n",
    "    A3 = f(z3)\n",
    "    y_hat = A3\n",
    "    return y_hat, z1, z2, z3\n",
    "def backpropagation(\n",
    "    y_hat,\n",
    "    f_prime,\n",
    "    W2, W3,\n",
    "    z1, z2, z3):\n",
    "    delta3 = np.multiply(y_hat - y, f_prime(z3))\n",
    "    delta2 = np.multiply(W3.T * delta3, f_prime(z2))\n",
    "    delta1 = np.multiply(W2.T * delta2, f_prime(z1))\n",
    "    return delta1, delta2, delta3\n",
    "def gradient_descent(\n",
    "    y, y_hat,\n",
    "    \n",
    "):\n",
    "    C = np.mean(np.power(y - y_hat, 2))\n",
    "    W1 -= delta1 * A0.T\n",
    "    W2 -= delta2 * A1.T\n",
    "    W3 -= delta3 * A2.T\n",
    "    b1 -= delta1\n",
    "    b2 -= delta2\n",
    "    b3 -= delta3\n",
    "for e in range(1, 200):\n",
    "    # Feedforward\n",
    "    A0 = f(X)\n",
    "    z1 = W1 * A0 + b1\n",
    "    A1 = f(z1)\n",
    "    z2 = W2 * A1 + b2\n",
    "    A2 = f(z2)\n",
    "    z3 = W3 * A2 + b3\n",
    "    A3 = f(z3)\n",
    "    y_hat = A3\n",
    "    \n",
    "    # Back Propagation\n",
    "    delta3 = np.multiply(y_hat - y, f_prime(z3))\n",
    "    delta2 = np.multiply(W3.T * delta3, f_prime(z2))\n",
    "    delta1 = np.multiply(W2.T * delta2, f_prime(z1))\n",
    "    \n",
    "    # Gradient Decent\n",
    "    C = np.mean(np.power(y - y_hat, 2))\n",
    "    W1 -= delta1 * A0.T\n",
    "    W2 -= delta2 * A1.T\n",
    "    W3 -= delta3 * A2.T\n",
    "    b1 -= delta1\n",
    "    b2 -= delta2\n",
    "    b3 -= delta3\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print(\"At training epoch\", e, \"the cost is now\", C)\n",
    "        if C < 0.0001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a, b, c = (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n",
      "1 2 3\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "print(a, b, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
