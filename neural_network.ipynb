{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "% Macro to define the angle bracket convension\n",
    "% Usage:\n",
    "%    \\agl{subject}{epoch}{training datapoint}{layer}\n",
    "% Example:\n",
    "%    \\agl{x}{e}{t}{l}\n",
    "\\newcommand{\\agl}[4]{\\langle_{#2}{#1}_{#4}^{#3}\\rangle}\n",
    "\\newcommand{\\x}[1]{\\agl{\\vec{x}}{}{\\phantom{|}#1}{}}\n",
    "\\newcommand{\\y}[1]{\\agl{\\vec{y}}{}{\\phantom{|}#1}{}}\n",
    "\\newcommand{\\I}[2]{\\agl{I}{#1}{}{l}}\n",
    "\\newcommand{\\J}[2]{\\agl{J}{#1}{}{l}}\n",
    "\\newcommand{\\W}[2]{\\agl{\\bf{W}}{#1}{}{#2}}\n",
    "\\newcommand{\\b}[2]{\\agl{\\vec{b}}{#1}{}{#2}}\n",
    "\\newcommand{\\z}[3]{\\agl{\\vec{z}}{#1}{\\phantom{|}#2}{#3}}\n",
    "\\newcommand{\\a}[3]{\\agl{\\vec{a}}{#1}{\\phantom{|}#2}{#3}}\n",
    "\\newcommand{\\pred}[2]{\\agl{\\hat{y}}{#1}{\\phantom{|}#2}{}}\n",
    "\\newcommand{\\c}[2]{\\agl{C}{#1}{#2}{}}\n",
    "\\newcommand{\\d}[3]{\\agl{\\vec{\\delta}}{#1}{\\phantom{|}#2}{#3}}\n",
    "\\newcommand{\\C}[1]{\\agl{\\bar{C}}{#1}{}{}}\n",
    "\\newcommand{\\dcdW}[3]{\\frac{\\partial\\c{#1}{#2}}{\\partial\\W{#1}{#3}}}\n",
    "\\newcommand{\\dcdb}[3]{\\frac{\\partial\\c{#1}{#2}}{\\partial\\b{#1}{#3}}}\n",
    "\\newcommand{\\dCdW}[2]{\\frac{\\partial\\C{#1}}{\\partial\\W{#1}{#2}}}\n",
    "\\newcommand{\\dCdb}[2]{\\frac{\\partial\\C{#1}}{\\partial\\b{#1}{#2}}}\n",
    "$$\n",
    "## Explanation of the Mathmatics Notation\n",
    "\n",
    "The mathmatics required to do this isn't trivial, but what makes it complicated are the number of variables to track.  When going through the math you find that you'll quickly run out of places to denote indicies, you quickly lose track of where you are and whether your dealing with a vector, scalar, matrix, etc.  \n",
    "\n",
    "Here we'll define a new convention which will allow us better understand the mathmatics and communicate the concepts via the language of mathmatics.  The theories and math behind this is complicated enough, why complicate it further with confusing notation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Angle Bracket Convension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "\n",
    "Because of the large amount of items we'll be tracking, we will be running out of spaces to put indexes.  Also, since we don't want to confuse with existing convensions, such as superscripts for exponentiation, we'll be using angle brackets to indicate a custom convension.  \n",
    "\n",
    "For example the following: $x^t$, would indicate $x$ raised to the power of $t$. \n",
    "\n",
    "But we'll be using the following: $\\agl{x}{}{t}{}$ to indicate $x$ at index $t$ to help clarify that we mean to index something rather than raise to a power.  \n",
    "\n",
    "Therefore, $\\agl{x}{}{t}{}^t$ would mean $x$ at index $t$ raised to the power of $t$.  \n",
    "\n",
    "Going further, we'll also define our convention to indicate that the position of the superscripts and subscripts to indicate which index we mean.  \n",
    "Here, we'll use the subscript that appears before the letter to indicate the training epoch, the superscript will be used to indicate the training datapoint, and the subscript which appears after the letter to indicate the layer in the network.\n",
    "\n",
    "For example: $\\agl{x}{e}{t}{l}$ means $x$ in training epoch $e$, for training datapoint $t$, at layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalars, Vectors, and Matricies\n",
    "\n",
    "In addition to the indicies, we'll also use certain letters to help us destinguish between scalars, vectors, and matricies.  This will help keep our equations easy to understand.\n",
    "- ALL matricies will be denoted with a capital letter with a bold fontface.  \n",
    "  For example: $\\bf M$  \n",
    "\n",
    "- ALL vectors will be denoted with a lowercase letter with an arrow or hat accent mark.  \n",
    "  For example: $\\vec v$, and $\\hat v$\n",
    "\n",
    "- If it's not indicated as a matrix or vector, then it can safely be assumed to be a scalar.  \n",
    "  For example:\n",
    " - $M$ is not a matrix, but a scalar, because though it's a capital letter, it's NOT bold. \n",
    " - $v$ and $\\bar v$ are not a vectors, but a scalars, because the lack of an accent mark or the accent mark is NOT an arrow or hat, but a bar.\n",
    " - and of course $x$ would be a scalar.\n",
    " \n",
    "#### Putting it all together\n",
    " \n",
    "Combining the notations we'll end up with something like $\\agl{\\bf M}{e}{t}{l}$, means matix $\\bf M$ in training epoch $e$, for training datapoint $t$, at layer $l$.\n",
    "\n",
    "Outside this angle bracket convenstion, regular mathmatical convension applies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix and Vector indexing Convension\n",
    "\n",
    "In addition to the angle bracket convension, we'll use another convension to indicate if we are refering to an index of a matrix or vector.  It'll be denoted by a letter or an angle bracket expression wrapped by square brackets, followed by a subscript.  \n",
    "\n",
    "A single subscript indicates a particular element of a vector and two subscripts seperated by a comma indicates a particular element of a matrix.  \n",
    "\n",
    "For example:\n",
    "\n",
    "- $[{\\bf M}]_{2,3}$ means element on row 2, column 3 in matrix $\\bf M$\n",
    "- $[{\\vec v}]_{2}$ means element on row 2 in vector $\\vec v$\n",
    "- However, something like $[{\\vec v}]_{2,3}$ is invalid because $\\vec v$ is a vector and doesn't have another dimension for $3$ to refer to.\n",
    "- And $[{\\bf M}]_{2}$ is also invalid because we need to specify a column on the matrix.\n",
    "- Also, it goes without saying that, $[x]_{2}$ and $[x]_{2,3}$ would be invalid because $x$ is a scalar.\n",
    "\n",
    "Don't forget, this convension also appies to bracket convension too: \n",
    "\n",
    "- $[\\agl{\\bf M}{}{}{}]_{2,3}$ means element on row 2, column 3 in matrix $\\agl{\\bf M}{}{}{}$\n",
    "- $[\\agl{\\vec v}{}{}{}]_{2}$ means element on row 2 in vector $\\agl{\\vec v}{}{}{}$\n",
    "- However, something like $[\\agl{\\vec v}{}{}{}]_{2,3}$ is invalid because $\\agl{\\vec v}{}{}{}$ is a vector and doesn't have another dimension for $3$ to refer to.\n",
    "- And $[\\agl{\\bf M}{}{}{}]_{2}$ is also invalid because we need to specify a column on the matrix.\n",
    "- Also, it goes without saying that, $[\\agl{x}{}{}{}]_{2}$ and $[\\agl{x}{}{}{}]_{2,3}$ would be invalid because $\\agl{x}{}{}{}$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "In order to track the increadible amount of items we'll define how we'll denote those items to help of keep track.  \n",
    "Yes, there is a total of 25 items to track at any given point, but with a properly organized system this will become a simple task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indecies\n",
    "\n",
    "- A particular training epoch will be indexed by $e$\n",
    "- A particular training datapoint will be indexed by $t$\n",
    "- A particular layer in the network will be indexed by $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "\n",
    "- The total number of training epochs will be denoted by $E$\n",
    "- The total number of training datapoints will be denoted by $T$\n",
    "- The total number of layers in the network will be denoted by $L$\n",
    "- The total number of rows at layer $l$ in training epoch $e$ will be denoted by $\\I{e}{l}$\n",
    "- The total number of columns at layer $l$ in training epoch $e$ will be denoted by $\\J{e}{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and Labels\n",
    "\n",
    "- Feature vector of a given training datapoint will be denoted by $\\x{t}$\n",
    "- Label vector corresponding to the feature vector of a given training datapoint will be denoted by $\\y{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activations, Weights, Biases, etc.\n",
    "\n",
    "- The activation function will be denoted by $f(z)$\n",
    "There are many activation functions that can be used such as $tanh$ (hyperbolic tangent), $ReLU$ (rectified linear unit), etc., however in this case we'll be using the sigmoid (a.k.a the logistic function).  \n",
    "The sigmoid is defined to be:\n",
    "$$f(z) = \\frac{1}{1+exp(-z)}$$\n",
    "- The derivative of the activation function will be denoted by $f^\\prime(z)$\n",
    "The derivative of the sigmoid is defined to be:\n",
    "$$f^\\prime(z) = \\frac{\\partial{f(z)}}{\\partial{z}} = f(z)\\cdot(1-f(z))$$\n",
    "***Note***: *We'll be using this for backpropagation.*\n",
    "- The weight matrix at layer $l$ in training epoch $e$ will be denoted by $\\W{e}{l}$\n",
    "- The bias vector at layer $l$ in training epoch $e$ will be denoted by $\\b{e}{l}$\n",
    "- Pre-activation vector at layer $l$, for training datapoint $t$, in epoch $e$ will be denoted by $\\z{e}{t}{l}$\n",
    "- Activation vector at layer $l$, for training datapoint $t$, in epoch $e$ will be denoted by $\\a{e}{t}{l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions, evaluations, and adjustments\n",
    "\n",
    "- Predicted label vector for datapoint $t$ in epoch $e$ will be denoted by $\\pred{e}{t}$\n",
    "- The cost for training datapoint $t$ in epoch $e$ will be denoted by $\\c{e}{t}$\n",
    "- The error vector at layer $l$, for training datapoint $t$, in epoch $e$ will be denoted by $\\d{e}{t}{l}$\n",
    "- The average cost for all training datapoints in epoch $e$ will be denoted by $\\C{e}$\n",
    "- The adjustment to the weight matrix needed for the next epoch to lower the cost for the particular training datapoint $t$ will be denoted by $\\dcdW{e}{t}{l}$\n",
    "- The adjustment to the bias vector needed for the next epoch to lower the cost for the particular training datapoint $t$ will be denoted by $\\dcdb{e}{t}{l}$\n",
    "- The adjustment to the weight matrix needed for the next epoch to lower the average cost across all training datapoints will be denoted by $\\dCdW{e}{l}$\n",
    "- The adjustment to the bias vector needed for the next epoch to lower the average cost across all training datapoints will be denoted by $\\dCdb{e}{l}$\n",
    "- The learning rate hyperparameter will be denoted by $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sanity Checks\n",
    "\n",
    "Run the following cells in this section to run some system checks.  \n",
    "If a potential issue is detected, warnings will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "import logging\n",
    "\n",
    "if sys.executable != subprocess.run(['which', 'python'], stdout=subprocess.PIPE, text=True).stdout.strip():\n",
    "    logging.warning('\\n    '.join(['',\n",
    "        'Kernel not set to use the same Python runtime as this notebook.',\n",
    "         'Change the kernel or else the remaining cells may not properly execute.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Funcitons\n",
    "\n",
    "Before we dive directly in the code, let's first create a few utility functions.  \n",
    "These will help us load the raw data from the `MNIST_digits` folder and visualize the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed to force replicable results\n",
    "np.random.seed(0)\n",
    "# Allow plots to be displayed inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def load_mnist_training_data():\n",
    "    # Load the training labels\n",
    "    training_labels = None\n",
    "    training_labels_path = os.getcwd() + '/MNIST_digits/train-labels-idx1-ubyte'\n",
    "    with open(training_labels_path, 'rb') as training_labels_file:\n",
    "        training_labels = np.frombuffer(\n",
    "            training_labels_file.read(\n",
    "                os.path.getsize(\n",
    "                    training_labels_path))[8:],\n",
    "            dtype=np.uint8)\n",
    "    # Load the training images\n",
    "    training_images = None\n",
    "    training_images_path = os.getcwd() + '/MNIST_digits/train-images-idx3-ubyte'\n",
    "    with open(training_images_path, 'rb') as training_images_file:\n",
    "        training_images = np.frombuffer(\n",
    "            training_images_file.read(\n",
    "                os.path.getsize(\n",
    "                    training_images_path))[16:],\n",
    "            dtype=np.uint8).reshape(60000, 28, 28)\n",
    "    # Return the raw Zipped result\n",
    "    return list(zip(training_labels, training_images))\n",
    "\n",
    "def load_mnist_testing_data():\n",
    "    # Load the testing labels\n",
    "    testing_labels = None\n",
    "    testing_labels_path = os.getcwd() + '/MNIST_digits/t10k-labels-idx1-ubyte'\n",
    "    with open(testing_labels_path, 'rb') as testing_labels_file:\n",
    "        testing_labels = np.frombuffer(\n",
    "            testing_labels_file.read(\n",
    "                os.path.getsize(\n",
    "                    testing_labels_path))[8:],\n",
    "            dtype=np.uint8)\n",
    "    # Load the testing images\n",
    "    testing_images = None\n",
    "    testing_images_path = os.getcwd() + '/MNIST_digits/t10k-images-idx3-ubyte'\n",
    "    with open(testing_images_path, 'rb') as testing_images_file:\n",
    "        testing_images = np.frombuffer(\n",
    "            testing_images_file.read(\n",
    "                os.path.getsize(testing_images_path))[16:],\n",
    "            dtype=np.uint8).reshape(10000,28,28)\n",
    "    # Return the raw Zipped result\n",
    "    return list(zip(testing_labels, testing_images))\n",
    "\n",
    "def view_image(image):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.axis('off')\n",
    "    imgplot = ax.imshow(image, cmap = mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    plt.show()\n",
    "\n",
    "def view_image_data(image):\n",
    "    for image_row in image:\n",
    "        print(' '.join([\n",
    "            str(image_row_item).rjust(3, ' ')\n",
    "            for image_row_item in image_row ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `load_mnist_training_data` function loads the training data from the `MNIST_digits`.\n",
    "- The `load_mnist_testing_data` function loads the testing data from the `MNIST_digits`.\n",
    "- The `view_image` function displays the image inline in the notebook.\n",
    "- The `view_image_data` function displays the actual 2D numpy array in a formatted way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Data\n",
    "\n",
    "Let's look at some data.  \n",
    "Let's look at `TRAINING_DATA[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title, image = TRAINING_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAINING_DATA[1] is the number', title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image \n",
    "Here is what you see, but..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...here is what the computer \"sees\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_image_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of a Basic Steps with Code\n",
    "\n",
    "In this step we'll run through the basic steps while coding along.  \n",
    "The basic steps are the following:  \n",
    "\n",
    "- Loading the data\n",
    "- Training\n",
    "  - Feedforward\n",
    "  - Backpropagation\n",
    "  - Gradient Descent\n",
    "- Prediction\n",
    "  - Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Training Data\n",
    "\n",
    "Before we can do anything we must first load the data.  \n",
    "Using the helper functions we defined earlier we'll run them and store the result in some constants.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training image data\n",
    "# A list of tuples of two items, where the first index of the tuple is the label (number) and the second is the image data (2D numpy array of grayscale image data) \n",
    "TRAINING_DATA = load_mnist_training_data()\n",
    "# Training image data\n",
    "# A list of tuples of two items, where the first index of the tuple is the label (number) and the second is the image data (2D numpy array of grayscale image data) \n",
    "TESTING_DATA = load_mnist_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label, image = TRAINING_DATA[0];\n",
    "X = np.matrix(image.reshape(784, 1)).astype(np.float64)\n",
    "y = np.matrix([1 if x == label else 0 for x in range(10)]).reshape(10, 1)\n",
    "\n",
    "W1 = np.matrix(np.random.randn(16, 784))\n",
    "W2 = np.matrix(np.random.randn(16, 16))\n",
    "W3 = np.matrix(np.random.randn(10, 16))\n",
    "\n",
    "b1 = np.matrix(np.random.randn(16, 1))\n",
    "b2 = np.matrix(np.random.randn(16, 1))\n",
    "b3 = np.matrix(np.random.randn(10, 1))\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = f(X)\n",
    "\n",
    "z1 = W1 * A0 + b1\n",
    "A1 = f(z1)\n",
    "\n",
    "z2 = W2 * A1 + b2\n",
    "A2 = f(z2)\n",
    "\n",
    "z3 = W3 * A2 + b3\n",
    "A3 = f(z3)\n",
    "\n",
    "y_hat = A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(z):\n",
    "    return np.multiply(f(z), (1 - f(z)))\n",
    "\n",
    "delta3 = np.multiply(y_hat - y, f_prime(z3))\n",
    "delta2 = np.multiply(W3.T * delta3, f_prime(z2))\n",
    "delta1 = np.multiply(W2.T * delta2, f_prime(z1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1 # Learning Rate\n",
    "C = np.mean(np.power(y - y_hat, 2))\n",
    "\n",
    "W1 -= eta * delta1 * A0.T\n",
    "W2 -= eta * delta2 * A1.T\n",
    "W3 -= eta * delta3 * A2.T\n",
    "\n",
    "b1 -= eta * delta1\n",
    "b2 -= eta * delta2\n",
    "b3 -= eta * delta3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(1, 200):\n",
    "    # Feedforward\n",
    "    A0 = f(X)\n",
    "    z1 = W1 * A0 + b1\n",
    "    A1 = f(z1)\n",
    "    z2 = W2 * A1 + b2\n",
    "    A2 = f(z2)\n",
    "    z3 = W3 * A2 + b3\n",
    "    A3 = f(z3)\n",
    "    y_hat = A3\n",
    "    \n",
    "    # Back Propagation\n",
    "    delta3 = np.multiply(y_hat - y, f_prime(z3))\n",
    "    delta2 = np.multiply(W3.T * delta3, f_prime(z2))\n",
    "    delta1 = np.multiply(W2.T * delta2, f_prime(z1))\n",
    "    \n",
    "    # Gradient Descent\n",
    "    C = np.mean(np.power(y - y_hat, 2))\n",
    "    W1 -= eta * delta1 * A0.T\n",
    "    W2 -= eta * delta2 * A1.T\n",
    "    W3 -= eta * delta3 * A2.T\n",
    "    b1 -= eta * delta1\n",
    "    b2 -= eta * delta2\n",
    "    b3 -= eta * delta3\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print(\"At training epoch\", e, \"the cost is now\", C)\n",
    "        if C < 0.0001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting seed to force replicable results\n",
    "np.random.seed(0)\n",
    "# Allow plots to be displayed inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "def load_mnist_training_data():\n",
    "    # Load the training labels\n",
    "    training_labels = None\n",
    "    training_labels_path = os.getcwd() + '/MNIST_digits/train-labels-idx1-ubyte'\n",
    "    with open(training_labels_path, 'rb') as training_labels_file:\n",
    "        training_labels = np.frombuffer(\n",
    "            training_labels_file.read(\n",
    "                os.path.getsize(\n",
    "                    training_labels_path))[8:],\n",
    "            dtype=np.uint8)\n",
    "    # Load the training images\n",
    "    training_images = None\n",
    "    training_images_path = os.getcwd() + '/MNIST_digits/train-images-idx3-ubyte'\n",
    "    with open(training_images_path, 'rb') as training_images_file:\n",
    "        training_images = np.frombuffer(\n",
    "            training_images_file.read(\n",
    "                os.path.getsize(\n",
    "                    training_images_path))[16:],\n",
    "            dtype=np.uint8).reshape(60000, 28, 28)\n",
    "    # Return the raw Zipped result\n",
    "    return list(zip(training_labels, training_images))\n",
    "\n",
    "def load_mnist_testing_data():\n",
    "    # Load the testing labels\n",
    "    testing_labels = None\n",
    "    testing_labels_path = os.getcwd() + '/MNIST_digits/t10k-labels-idx1-ubyte'\n",
    "    with open(testing_labels_path, 'rb') as testing_labels_file:\n",
    "        testing_labels = np.frombuffer(\n",
    "            testing_labels_file.read(\n",
    "                os.path.getsize(\n",
    "                    testing_labels_path))[8:],\n",
    "            dtype=np.uint8)\n",
    "    # Load the testing images\n",
    "    testing_images = None\n",
    "    testing_images_path = os.getcwd() + '/MNIST_digits/t10k-images-idx3-ubyte'\n",
    "    with open(testing_images_path, 'rb') as testing_images_file:\n",
    "        testing_images = np.frombuffer(\n",
    "            testing_images_file.read(\n",
    "                os.path.getsize(testing_images_path))[16:],\n",
    "            dtype=np.uint8).reshape(10000,28,28)\n",
    "    # Return the raw Zipped result\n",
    "    return list(zip(testing_labels, testing_images))\n",
    "\n",
    "def view_image(image):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.axis('off')\n",
    "    imgplot = ax.imshow(image, cmap = mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    plt.show()\n",
    "\n",
    "def view_image_data(image):\n",
    "    for image_row in image:\n",
    "        print(' '.join([\n",
    "            str(image_row_item).rjust(3, ' ')\n",
    "            for image_row_item in image_row ]))\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    # Hyperparameters\n",
    "    eta = 0.2\n",
    "    f = None\n",
    "    f_prime = None\n",
    "    # Network State\n",
    "    X = None\n",
    "    y = None\n",
    "    W1 = None\n",
    "    W2 = None\n",
    "    W3 = None\n",
    "    b1 = None\n",
    "    b2 = None\n",
    "    b3 = None\n",
    "    # Feedforward intermediates\n",
    "    A0 = None\n",
    "    A1 = None\n",
    "    A2 = None\n",
    "    A3 = None\n",
    "    z1 = None\n",
    "    z2 = None\n",
    "    z3 = None\n",
    "    y_hat = None\n",
    "    # Backpropagation\n",
    "    delta1 = None\n",
    "    delta2 = None\n",
    "    delta3 = None\n",
    "    # Gradient Descent\n",
    "    C = None\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.A0 = self.f(self.X)\n",
    "        self.z1 = self.W1 * self.A0 + self.b1\n",
    "        self.A1 = self.f(self.z1)\n",
    "        self.z2 = self.W2 * self.A1 + self.b2\n",
    "        self.A2 = self.f(self.z2)\n",
    "        self.z3 = self.W3 * self.A2 + self.b3\n",
    "        self.A3 = self.f(self.z3)\n",
    "        self.y_hat = self.A3\n",
    "    def backpropagation(self):\n",
    "        self.delta3 = np.multiply(self.y_hat - self.y, self.f_prime(self.z3))\n",
    "        self.delta2 = np.multiply(self.W3.T * self.delta3, self.f_prime(self.z2))\n",
    "        self.delta1 = np.multiply(self.W2.T * self.delta2, self.f_prime(self.z1))\n",
    "    def gradient_descent(self):\n",
    "        self.C = np.mean(np.power(self.y - self.y_hat, 2))\n",
    "        self.W1 -= self.eta * self.delta1 * self.A0.T\n",
    "        self.W2 -= self.eta * self.delta2 * self.A1.T\n",
    "        self.W3 -= self.eta * self.delta3 * self.A2.T\n",
    "        self.b1 -= self.eta * self.delta1\n",
    "        self.b2 -= self.eta * self.delta2\n",
    "        self.b3 -= self.eta * self.delta3\n",
    "    def predict(self, X):\n",
    "        A0 = self.f(X)\n",
    "        z1 = self.W1 * A0 + self.b1\n",
    "        A1 = self.f(z1)\n",
    "        z2 = self.W2 * A1 + self.b2\n",
    "        A2 = self.f(z2)\n",
    "        z3 = self.W3 * A2 + self.b3\n",
    "        A3 = self.f(z3)\n",
    "        y_hat = A3\n",
    "        return y_hat\n",
    "\n",
    "# Sigmoid\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "# Derivative of sigmoid\n",
    "def f_prime(z):\n",
    "    return np.multiply(f(z), (1 - f(z)))\n",
    "\n",
    "# Training image data\n",
    "# A list of tuples of two items, where the first index of the tuple is the label (number) and the second is the image data (2D numpy array of grayscale image data) \n",
    "TRAINING_DATA = load_mnist_training_data()\n",
    "# Training image data\n",
    "# A list of tuples of two items, where the first index of the tuple is the label (number) and the second is the image data (2D numpy array of grayscale image data) \n",
    "TESTING_DATA = load_mnist_testing_data()    \n",
    "    \n",
    "\n",
    "\n",
    "E = 10000 # total number of epochs\n",
    "T = 100 # total number of data points (max 60000)\n",
    "nn = NeuralNetwork()\n",
    "nn.f = f\n",
    "nn.f_prime = f_prime\n",
    "\n",
    "nn.W1 = np.matrix(np.random.randn(16, 784))\n",
    "nn.b1 = np.matrix(np.random.randn(16, 1))\n",
    "nn.W2 = np.matrix(np.random.randn(16, 16))\n",
    "nn.b2 = np.matrix(np.random.randn(16, 1))\n",
    "nn.W3 = np.matrix(np.random.randn(10, 16))\n",
    "nn.b3 = np.matrix(np.random.randn(10, 1))\n",
    "\n",
    "for e in range(E):\n",
    "    for t in range(T):\n",
    "        label, image = TRAINING_DATA[t]\n",
    "        nn.X = np.matrix(image.reshape(784, 1)).astype(np.float64)\n",
    "        nn.y = np.matrix([1 if x == label else 0 for x in range(10)]).reshape(10, 1)\n",
    "        nn.feedforward()\n",
    "        nn.backpropagation()\n",
    "        nn.gradient_descent()\n",
    "    if e % 100 == 0:\n",
    "        print('At training epoch', e, 'the cost is now', '{:.6f}'.format(nn.C))\n",
    "    if nn.C < 0.0001:\n",
    "        print('Stopping training at epoch', e, 'with cost', '{:.6f}'.format(nn.C))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    label, image = TRAINING_DATA[t]\n",
    "    X = np.matrix(image.reshape(784, 1)).astype(np.float64)\n",
    "    y = np.matrix([1 if x == label else 0 for x in range(10)]).reshape(10, 1)\n",
    "\n",
    "    result = nn.predict(X)\n",
    "    result_label = list(np.array(result).reshape(10))\n",
    "    result_label = list(np.array(result_label).reshape(10)).index(max(result_label))\n",
    "    print(\n",
    "        'testing for', label,\n",
    "        'result_label', result_label,\n",
    "        'correct?', 'YES' if label == result_label else 'NO',\n",
    "    )\n",
    "    print(' | '.join(['{} => {:.1f}%'.format(i, p*100) for i, p in zip(range(10), list(np.array(result).reshape(10)))]))\n",
    "    view_image(image)\n",
    "    print('------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
